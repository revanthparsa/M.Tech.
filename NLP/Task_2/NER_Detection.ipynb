{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment2_Part1ab.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8YiDi3cui9C3",
        "PxYO7q8AQRjW",
        "E9J8nmSPjGPw",
        "Vd1zZOnqjS2a",
        "bwmAzbl4beLZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YiDi3cui9C3"
      },
      "source": [
        "## Loading required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByYQydTZL6qz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e67244e-93d3-44ed-a0a3-456957dccc7f"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.tag import StanfordNERTagger\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "import spacy \n",
        "from spacy import displacy\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxYO7q8AQRjW"
      },
      "source": [
        "## Required Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW0JZRBEQV5U"
      },
      "source": [
        "def decontracted(phrase):\n",
        "    ''' \n",
        "      Function used to decontact the words in the phrase\n",
        "      Input: phrase\n",
        "      Output: decontracted phrase\n",
        "    '''\n",
        "    phrase = re.sub(r\"\\S+@\\S+\",\" \",phrase)\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    phrase = re.sub(r\"  \", \" \", phrase)\n",
        "    \n",
        "    return phrase\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDEcm57_EGny",
        "outputId": "d69c51e1-b467-4600-b735-1aba0288fd0f"
      },
      "source": [
        "stop_words=stopwords.words('english')\n",
        "temp = ['we','THE']\n",
        "stop_words = stop_words+temp\n",
        "for i in stop_words:\n",
        "  print(i)\n",
        "  temp.append(i[0].upper()+i[1:])\n",
        "  temp.append(i.upper())\n",
        "stop_words = stop_words+temp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i\n",
            "me\n",
            "my\n",
            "myself\n",
            "we\n",
            "our\n",
            "ours\n",
            "ourselves\n",
            "you\n",
            "you're\n",
            "you've\n",
            "you'll\n",
            "you'd\n",
            "your\n",
            "yours\n",
            "yourself\n",
            "yourselves\n",
            "he\n",
            "him\n",
            "his\n",
            "himself\n",
            "she\n",
            "she's\n",
            "her\n",
            "hers\n",
            "herself\n",
            "it\n",
            "it's\n",
            "its\n",
            "itself\n",
            "they\n",
            "them\n",
            "their\n",
            "theirs\n",
            "themselves\n",
            "what\n",
            "which\n",
            "who\n",
            "whom\n",
            "this\n",
            "that\n",
            "that'll\n",
            "these\n",
            "those\n",
            "am\n",
            "is\n",
            "are\n",
            "was\n",
            "were\n",
            "be\n",
            "been\n",
            "being\n",
            "have\n",
            "has\n",
            "had\n",
            "having\n",
            "do\n",
            "does\n",
            "did\n",
            "doing\n",
            "a\n",
            "an\n",
            "the\n",
            "and\n",
            "but\n",
            "if\n",
            "or\n",
            "because\n",
            "as\n",
            "until\n",
            "while\n",
            "of\n",
            "at\n",
            "by\n",
            "for\n",
            "with\n",
            "about\n",
            "against\n",
            "between\n",
            "into\n",
            "through\n",
            "during\n",
            "before\n",
            "after\n",
            "above\n",
            "below\n",
            "to\n",
            "from\n",
            "up\n",
            "down\n",
            "in\n",
            "out\n",
            "on\n",
            "off\n",
            "over\n",
            "under\n",
            "again\n",
            "further\n",
            "then\n",
            "once\n",
            "here\n",
            "there\n",
            "when\n",
            "where\n",
            "why\n",
            "how\n",
            "all\n",
            "any\n",
            "both\n",
            "each\n",
            "few\n",
            "more\n",
            "most\n",
            "other\n",
            "some\n",
            "such\n",
            "no\n",
            "nor\n",
            "not\n",
            "only\n",
            "own\n",
            "same\n",
            "so\n",
            "than\n",
            "too\n",
            "very\n",
            "s\n",
            "t\n",
            "can\n",
            "will\n",
            "just\n",
            "don\n",
            "don't\n",
            "should\n",
            "should've\n",
            "now\n",
            "d\n",
            "ll\n",
            "m\n",
            "o\n",
            "re\n",
            "ve\n",
            "y\n",
            "ain\n",
            "aren\n",
            "aren't\n",
            "couldn\n",
            "couldn't\n",
            "didn\n",
            "didn't\n",
            "doesn\n",
            "doesn't\n",
            "hadn\n",
            "hadn't\n",
            "hasn\n",
            "hasn't\n",
            "haven\n",
            "haven't\n",
            "isn\n",
            "isn't\n",
            "ma\n",
            "mightn\n",
            "mightn't\n",
            "mustn\n",
            "mustn't\n",
            "needn\n",
            "needn't\n",
            "shan\n",
            "shan't\n",
            "shouldn\n",
            "shouldn't\n",
            "wasn\n",
            "wasn't\n",
            "weren\n",
            "weren't\n",
            "won\n",
            "won't\n",
            "wouldn\n",
            "wouldn't\n",
            "we\n",
            "THE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHtk6W8r_GC4"
      },
      "source": [
        "def pipeline(data): \n",
        "\n",
        "  processed_data = []\n",
        "  for sentence in tqdm(data):\n",
        "  \n",
        "    sentence = decontracted(sentence) #decontact the sentence in the review\n",
        "    sentence = re.sub('[^A-Za-z]+', ' ', sentence) #retaining only alphabets in the sentence \n",
        "\n",
        "    #Word tokenization\n",
        "    word_tokens = word_tokenize(sentence) \n",
        "\n",
        "    #Stop word removal\n",
        "    filtered_sentence = [w for w in word_tokens if  w not in stop_words]\n",
        "    \n",
        "    #Lemmatization\n",
        "    wnl = WordNetLemmatizer()\n",
        "    filtered_sentence = [wnl.lemmatize(w) for w in filtered_sentence ]\n",
        "\n",
        "    final = ' '.join(filtered_sentence)\n",
        "    processed_data.append(final)\n",
        "  return processed_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRjBfbPEHOXD"
      },
      "source": [
        "def frequency(lst):\n",
        "  my_dict = {}\n",
        "  for i in tqdm(range(len(lst))):\n",
        "    # To not consider the single character tags\n",
        "    if len(lst[i]) != 1:              \n",
        "      # To check the tag in the dictionary or not  \n",
        "      if lst[i] not in my_dict.keys():  \n",
        "        # To check the underscore in the tag\n",
        "        if (\"_\" in lst[i]):            \n",
        "          l = lst[i].split('_')\n",
        "          for j in range(len(l)):\n",
        "            # For all the words in the tag count only the already existing word      \n",
        "            if l[j] != \"New\":\n",
        "              # Check only the tags which do not have 'New' in it because we can \n",
        "              # have new york and new zealand which are different but if we do not \n",
        "              # filter it will consider as same word\n",
        "              if l[j] in my_dict.keys():\n",
        "                my_dict[l[j]] += 1\n",
        "            else:\n",
        "                my_dict[lst[i]] = 1\n",
        "        else:\n",
        "          my_dict[lst[i]] = 1\n",
        "      else:\n",
        "        my_dict[lst[i]] += 1\n",
        "  # Sort in the decreasing order of the counts\n",
        "  lst_s = sorted(my_dict.items(), key=lambda x: x[1], reverse= True) \n",
        "  return lst_s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9J8nmSPjGPw"
      },
      "source": [
        "## Downloading the 20newsgroup dataset from sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSNk8EyuRdkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "424c66d3-2b65-438a-d004-06df1c3eed98"
      },
      "source": [
        "twenty_data = fetch_20newsgroups(subset='all', shuffle=False, remove=('headers', 'quotes'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHe1kioOmu4n",
        "outputId": "0ba63232-1f61-43ad-a211-8d5409335a87"
      },
      "source": [
        "len(twenty_data.target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18846"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpPFDHJpR7LV"
      },
      "source": [
        "#twenty_data.target_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd1zZOnqjS2a"
      },
      "source": [
        "## Cleaning the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZS4WAmNA2QE",
        "outputId": "cf9678a6-dbec-4349-879a-7daaf5b198f1"
      },
      "source": [
        "data = pipeline(twenty_data.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 18846/18846 [00:42<00:00, 444.69it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtS3cbay1QXm",
        "outputId": "23137bd3-97fb-4a00-e33b-918bf95e2dbe"
      },
      "source": [
        "data[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'morgan guzman era run higher last year cub idiot pitch harkey much hibbard castillo good think stud pitcher season far Morgan Guzman helped lead Cubs top ERA even better rotation Atlanta Cubs ERA Braves know early season Cubs fan learned enjoy short triumph still'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3kZeBWde2Ct"
      },
      "source": [
        "## Named Entities using Stanford NER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwmAzbl4beLZ"
      },
      "source": [
        "### Downloading stanford NER package and loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO0loHXpTgH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "128fed18-b90e-4101-caa3-ba3d52753a89"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1N0xYCtTK12H83-4avQugpM_JuoJXbsT8\n",
        "!unzip './stanford-ner-4.2.0.zip'\n",
        "!rm './stanford-ner-4.2.0.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1N0xYCtTK12H83-4avQugpM_JuoJXbsT8\n",
            "To: /content/stanford-ner-4.2.0.zip\n",
            "180MB [00:03, 49.7MB/s]\n",
            "Archive:  ./stanford-ner-4.2.0.zip\n",
            "   creating: stanford-ner-2020-11-17/\n",
            "   creating: stanford-ner-2020-11-17/lib/\n",
            "  inflating: stanford-ner-2020-11-17/lib/jollyday-0.4.9.jar  \n",
            "  inflating: stanford-ner-2020-11-17/lib/stanford-ner-resources.jar  \n",
            "  inflating: stanford-ner-2020-11-17/lib/joda-time.jar  \n",
            "  inflating: stanford-ner-2020-11-17/stanford-ner-4.2.0.jar  \n",
            "  inflating: stanford-ner-2020-11-17/NERDemo.java  \n",
            "  inflating: stanford-ner-2020-11-17/LICENSE.txt  \n",
            "  inflating: stanford-ner-2020-11-17/sample-conll-file.txt  \n",
            "  inflating: stanford-ner-2020-11-17/stanford-ner-4.2.0-javadoc.jar  \n",
            "  inflating: stanford-ner-2020-11-17/stanford-ner-4.2.0-sources.jar  \n",
            "  inflating: stanford-ner-2020-11-17/stanford-ner.jar  \n",
            "  inflating: stanford-ner-2020-11-17/sample.txt  \n",
            "  inflating: stanford-ner-2020-11-17/build.xml  \n",
            "  inflating: stanford-ner-2020-11-17/ner-gui.bat  \n",
            "  inflating: stanford-ner-2020-11-17/sample-w-time.txt  \n",
            "  inflating: stanford-ner-2020-11-17/ner-gui.command  \n",
            "   creating: stanford-ner-2020-11-17/classifiers/\n",
            "  inflating: stanford-ner-2020-11-17/classifiers/english.muc.7class.distsim.prop  \n",
            "  inflating: stanford-ner-2020-11-17/classifiers/english.all.3class.distsim.crf.ser.gz  \n",
            "  inflating: stanford-ner-2020-11-17/classifiers/english.conll.4class.distsim.crf.ser.gz  \n",
            "  inflating: stanford-ner-2020-11-17/classifiers/example.serialized.ncc.prop  \n",
            "  inflating: stanford-ner-2020-11-17/classifiers/english.muc.7class.distsim.crf.ser.gz  \n",
            "  inflating: stanford-ner-2020-11-17/classifiers/english.all.3class.distsim.prop  \n",
            "  inflating: stanford-ner-2020-11-17/classifiers/example.serialized.ncc.ncc.ser.gz  \n",
            "  inflating: stanford-ner-2020-11-17/classifiers/english.conll.4class.distsim.prop  \n",
            "  inflating: stanford-ner-2020-11-17/ner-gui.sh  \n",
            "  inflating: stanford-ner-2020-11-17/sample.ner.txt  \n",
            "  inflating: stanford-ner-2020-11-17/ner.bat  \n",
            "  inflating: stanford-ner-2020-11-17/README.txt  \n",
            "  inflating: stanford-ner-2020-11-17/ner.sh  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wekOHuvZJUo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5f93370-63be-433b-edd4-88790c415b12"
      },
      "source": [
        "model = './stanford-ner-2020-11-17/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
        "jar = './stanford-ner-2020-11-17/stanford-ner.jar'\n",
        "\n",
        "tagger = StanfordNERTagger(model, jar,encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/tag/stanford.py:183: DeprecationWarning: \n",
            "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
            "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
            "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alHTqe13tVFj"
      },
      "source": [
        "### Entities and Labels list creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkl0-HbomemB",
        "outputId": "b4f226ed-644a-4fd7-b079-e1a887dae24c"
      },
      "source": [
        "lst_list_data = []\n",
        "for i in range(0,10):\n",
        "  lst_data = ''\n",
        "  for j in tqdm(data[int(len(data)*0.1*i) : int(len(data)*0.1*(i+1))]):\n",
        "    lst_data = lst_data + j\n",
        "  lst_list_data.append(lst_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1884/1884 [00:00<00:00, 201331.72it/s]\n",
            "100%|██████████| 1885/1885 [00:00<00:00, 393738.20it/s]\n",
            "100%|██████████| 1884/1884 [00:00<00:00, 381724.01it/s]\n",
            "100%|██████████| 1885/1885 [00:00<00:00, 297216.76it/s]\n",
            "100%|██████████| 1885/1885 [00:00<00:00, 392877.31it/s]\n",
            "100%|██████████| 1884/1884 [00:00<00:00, 392571.35it/s]\n",
            "100%|██████████| 1885/1885 [00:00<00:00, 400074.03it/s]\n",
            "100%|██████████| 1884/1884 [00:00<00:00, 408206.88it/s]\n",
            "100%|██████████| 1885/1885 [00:00<00:00, 444946.99it/s]\n",
            "100%|██████████| 1885/1885 [00:00<00:00, 505838.97it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBWf2aqF9tlZ",
        "outputId": "5cecc4fe-9948-431b-bead-1c3fa7db5c26"
      },
      "source": [
        "NER = []\n",
        "for i in tqdm(lst_list_data):\n",
        "  words = nltk.word_tokenize(i)\n",
        "  tagged = tagger.tag(words)\n",
        "  NER = NER + tagged"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [03:27<00:00, 20.74s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsXohq0-czOV"
      },
      "source": [
        "Removing 'O' tagged NER i.e NER with background tag for words that did not fit any of the named entity category labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNZrpaqSfbMz"
      },
      "source": [
        "final_NER = []\n",
        "for i in NER:\n",
        "  if((i[1] != 'O')):\n",
        "    final_NER.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv2n2DHpj237"
      },
      "source": [
        "### Finding the top 100 LOC and PERSON entities from the data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAjJvfoJLCT3"
      },
      "source": [
        "#Finding Location and person Entity\n",
        "loc = []\n",
        "person = []\n",
        "for i in final_NER:\n",
        "  if(i[1]== 'LOCATION'):\n",
        "    loc.append(i[0])\n",
        "  if(i[1]== 'PERSON'):\n",
        "    person.append(i[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sen37SRNlzSD"
      },
      "source": [
        "# The words having spaces will be joined using underscore\n",
        "for i in range(len(loc)):\n",
        "  if (\" \" in loc[i]):\n",
        "    loc[i] = loc[i].replace(\" \", \"_\")\n",
        "for i in range(len(person)):\n",
        "  if (\" \" in person[i]):\n",
        "    person[i] = person[i].replace(\" \", \"_\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T3gX6c3zjZl",
        "outputId": "082cb3f8-d185-469a-de7b-1be5e55a0de9"
      },
      "source": [
        "# Use the frequency function defined in the code to get the tags its corresponding count in the dataset\n",
        "lst_loc_stan = frequency(loc)  \n",
        "lst_person_stan = frequency(person)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 22137/22137 [00:00<00:00, 778623.60it/s]\n",
            "100%|██████████| 75187/75187 [00:00<00:00, 1114245.61it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4fPGfCQztX6"
      },
      "source": [
        "lst_loc_freq_s = []\n",
        "lst_loc_entity_s = []\n",
        "lst_person_freq_s = []\n",
        "lst_person_entity_s = []\n",
        "for i in range(100):\n",
        "  a, b = lst_loc_stan[i]\n",
        "  c, d = lst_person_stan[i]\n",
        "  lst_loc_freq_s.append(b)\n",
        "  lst_loc_entity_s.append(a)\n",
        "  lst_person_freq_s.append(d)\n",
        "  lst_person_entity_s.append(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "gG421PM9hOuP",
        "outputId": "cb4f4254-3340-48cf-937f-f9ce374e2687"
      },
      "source": [
        "loc_person_NER_stan = {'Location': lst_loc_entity_s, 'Location Frequency': lst_loc_freq_s,\n",
        "                  'Person': lst_person_entity_s, 'Person Frequency': lst_person_freq_s}\n",
        "df_stan = pd.DataFrame(loc_person_NER_stan)\n",
        "df_stan"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Location</th>\n",
              "      <th>Location Frequency</th>\n",
              "      <th>Person</th>\n",
              "      <th>Person Frequency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Israel</td>\n",
              "      <td>848</td>\n",
              "      <td>Jesus</td>\n",
              "      <td>1190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>US</td>\n",
              "      <td>811</td>\n",
              "      <td>David</td>\n",
              "      <td>889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Armenia</td>\n",
              "      <td>409</td>\n",
              "      <td>John</td>\n",
              "      <td>851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>New</td>\n",
              "      <td>406</td>\n",
              "      <td>Paul</td>\n",
              "      <td>598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>United</td>\n",
              "      <td>380</td>\n",
              "      <td>Clinton</td>\n",
              "      <td>457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>Norway</td>\n",
              "      <td>41</td>\n",
              "      <td>Dick</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>London</td>\n",
              "      <td>40</td>\n",
              "      <td>Carl</td>\n",
              "      <td>83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>Houston</td>\n",
              "      <td>40</td>\n",
              "      <td>Sam</td>\n",
              "      <td>83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Pluto</td>\n",
              "      <td>40</td>\n",
              "      <td>Tommy</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Ohio</td>\n",
              "      <td>40</td>\n",
              "      <td>Randy</td>\n",
              "      <td>81</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Location  Location Frequency   Person  Person Frequency\n",
              "0    Israel                 848    Jesus              1190\n",
              "1        US                 811    David               889\n",
              "2   Armenia                 409     John               851\n",
              "3       New                 406     Paul               598\n",
              "4    United                 380  Clinton               457\n",
              "..      ...                 ...      ...               ...\n",
              "95   Norway                  41     Dick                84\n",
              "96   London                  40     Carl                83\n",
              "97  Houston                  40      Sam                83\n",
              "98    Pluto                  40    Tommy                82\n",
              "99     Ohio                  40    Randy                81\n",
              "\n",
              "[100 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEdfa-jam0em"
      },
      "source": [
        "df_stan.to_csv('Top_100_Stan.csv') #saving the top 100 location and person entities "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3PhrnRffeFL"
      },
      "source": [
        "## Named Entities using Spacy NER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71jTbG5bcSUz"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZ5ltqVOnMf2",
        "outputId": "48d15a4f-0428-4b88-b772-24f049ebe048"
      },
      "source": [
        "ent_spacy = []\n",
        "lab_spacy = []\n",
        "for text in tqdm(data):\n",
        "  doc = nlp(text)\n",
        "  for ent in doc.ents:\n",
        "    ent_spacy.append(ent)\n",
        "    lab_spacy.append(ent.label_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 18846/18846 [21:49<00:00, 14.39it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9QtdDkuhRug"
      },
      "source": [
        "### Finding the top 100 LOC and PERSON entities from the data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmSfueR1qQSh",
        "outputId": "8e17a395-f78c-4232-f00e-1631ca76d00e"
      },
      "source": [
        "Loc = []\n",
        "Person = []\n",
        "for i in tqdm(range(len(ent_spacy))):\n",
        "  if(lab_spacy[i] == 'LOC' or lab_spacy[i] == 'GPE'):\n",
        "    Loc.append(str(ent_spacy[i]))\n",
        "  if(lab_spacy[i] == 'PERSON'):\n",
        "    Person.append(str(ent_spacy[i]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 202183/202183 [00:00<00:00, 367428.37it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga28tOerXmDM"
      },
      "source": [
        "# The words having spaces will be joined using underscore\n",
        "for i in range(len(Loc)):\n",
        "  if (\" \" in Loc[i]):\n",
        "    Loc[i] = Loc[i].replace(\" \", \"_\")\n",
        "for i in range(len(Person)):\n",
        "  if (\" \" in Person[i]):\n",
        "    Person[i] = Person[i].replace(\" \", \"_\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjQAP6XmAsGw",
        "outputId": "ee5066fe-3609-4910-9bfe-6cada1056838"
      },
      "source": [
        "# Use the frequency function defined in the code to get the tags its corresponding count in the dataset\n",
        "lst_loc = frequency(Loc)  \n",
        "lst_person = frequency(Person)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 18876/18876 [00:00<00:00, 713946.62it/s]\n",
            "100%|██████████| 53738/53738 [00:00<00:00, 535634.78it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bYM_iePL4bA"
      },
      "source": [
        "lst_loc_freq = []\n",
        "lst_loc_entity = []\n",
        "lst_person_freq = []\n",
        "lst_person_entity = []\n",
        "for i in range(100):\n",
        "  a, b = lst_loc[i]\n",
        "  c, d = lst_person[i]\n",
        "  lst_loc_freq.append(b)\n",
        "  lst_loc_entity.append(a)\n",
        "  lst_person_freq.append(d)\n",
        "  lst_person_entity.append(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "ozR10b-IeH7s",
        "outputId": "21c9f7d0-2bbe-4941-818f-618d62f80ad5"
      },
      "source": [
        "loc_person_NER = {'Location': lst_loc_entity, 'Location Frequency': lst_loc_freq,\n",
        "                  'Person': lst_person_entity, 'Person Frequency': lst_person_freq}\n",
        "df = pd.DataFrame(loc_person_NER)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Location</th>\n",
              "      <th>Location Frequency</th>\n",
              "      <th>Person</th>\n",
              "      <th>Person Frequency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Israel</td>\n",
              "      <td>809</td>\n",
              "      <td>Jesus</td>\n",
              "      <td>1347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>US</td>\n",
              "      <td>804</td>\n",
              "      <td>John</td>\n",
              "      <td>1058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Earth</td>\n",
              "      <td>338</td>\n",
              "      <td>David</td>\n",
              "      <td>996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Turkey</td>\n",
              "      <td>324</td>\n",
              "      <td>Paul</td>\n",
              "      <td>653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Canada</td>\n",
              "      <td>262</td>\n",
              "      <td>Mike</td>\n",
              "      <td>586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>England</td>\n",
              "      <td>37</td>\n",
              "      <td>Sam</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Austria</td>\n",
              "      <td>37</td>\n",
              "      <td>Roy</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>IL</td>\n",
              "      <td>36</td>\n",
              "      <td>Adams</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Cambridge</td>\n",
              "      <td>36</td>\n",
              "      <td>Dale</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Istanbul</td>\n",
              "      <td>36</td>\n",
              "      <td>JFIF</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Location  Location Frequency Person  Person Frequency\n",
              "0      Israel                 809  Jesus              1347\n",
              "1          US                 804   John              1058\n",
              "2       Earth                 338  David               996\n",
              "3      Turkey                 324   Paul               653\n",
              "4      Canada                 262   Mike               586\n",
              "..        ...                 ...    ...               ...\n",
              "95    England                  37    Sam                86\n",
              "96    Austria                  37    Roy                84\n",
              "97         IL                  36  Adams                84\n",
              "98  Cambridge                  36   Dale                84\n",
              "99   Istanbul                  36   JFIF                82\n",
              "\n",
              "[100 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImYNC71jNnYu"
      },
      "source": [
        "df.to_csv('Top_100_Spacy.csv') #saving the top 100 location and person entities "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbg4S6QQnj14"
      },
      "source": [
        "## Degree of correlation between stanford NER and Spacy NER on generated LOCATION and PERSON entities "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG2TyZbimvrD"
      },
      "source": [
        "count_loc = 0\n",
        "count_person = 0\n",
        "m_words_loc = []\n",
        "m_word_freq_stan_loc = []\n",
        "m_word_freq_spacy_loc = []\n",
        "m_words_person = []\n",
        "m_word_freq_stan_person = []\n",
        "m_word_freq_spacy_person = []\n",
        "for i in range(100):\n",
        "  if lst_loc_entity[i] in lst_loc_entity_s:\n",
        "    count_loc += 1 \n",
        "    m_words_loc.append(lst_loc_entity[i])\n",
        "    m_word_freq_stan_loc.append(lst_loc_freq_s[i])\n",
        "    m_word_freq_spacy_loc.append(lst_loc_freq[i])\n",
        "  if lst_person_entity [i] in lst_person_entity_s:\n",
        "    count_person += 1 \n",
        "    m_words_person.append(lst_person_entity[i])\n",
        "    m_word_freq_stan_person.append(lst_person_freq_s[i])\n",
        "    m_word_freq_spacy_person.append(lst_person_freq[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "z5mpcB9kqVHA",
        "outputId": "4135a4a4-0db2-44b1-b617-bc905ba830ea"
      },
      "source": [
        "loc_match_NER = {'Location Entity': m_words_loc, 'Frequency_Stanford_NER': m_word_freq_stan_loc,\n",
        "                  'Frequency_Spacy_NER': m_word_freq_spacy_loc}\n",
        "df_m_loc = pd.DataFrame(loc_match_NER)\n",
        "df_m_loc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Location Entity</th>\n",
              "      <th>Frequency_Stanford_NER</th>\n",
              "      <th>Frequency_Spacy_NER</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Israel</td>\n",
              "      <td>848</td>\n",
              "      <td>809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>US</td>\n",
              "      <td>811</td>\n",
              "      <td>804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Earth</td>\n",
              "      <td>409</td>\n",
              "      <td>338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Turkey</td>\n",
              "      <td>406</td>\n",
              "      <td>324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Canada</td>\n",
              "      <td>380</td>\n",
              "      <td>262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>Italy</td>\n",
              "      <td>44</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>Cyprus</td>\n",
              "      <td>43</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>England</td>\n",
              "      <td>41</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>Cambridge</td>\n",
              "      <td>40</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>Istanbul</td>\n",
              "      <td>40</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>71 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Location Entity  Frequency_Stanford_NER  Frequency_Spacy_NER\n",
              "0           Israel                     848                  809\n",
              "1               US                     811                  804\n",
              "2            Earth                     409                  338\n",
              "3           Turkey                     406                  324\n",
              "4           Canada                     380                  262\n",
              "..             ...                     ...                  ...\n",
              "66           Italy                      44                   39\n",
              "67          Cyprus                      43                   38\n",
              "68         England                      41                   37\n",
              "69       Cambridge                      40                   36\n",
              "70        Istanbul                      40                   36\n",
              "\n",
              "[71 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "LsnllF2IqxRz",
        "outputId": "5976985a-efd8-45e5-9800-f906a5044611"
      },
      "source": [
        "person_match_NER = {'PERSON Entity': m_words_person, 'Frequency_Stanford_NER': m_word_freq_stan_person,\n",
        "                  'Frequency_Spacy_NER': m_word_freq_spacy_person}\n",
        "df_m_person = pd.DataFrame(person_match_NER)\n",
        "df_m_person"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PERSON Entity</th>\n",
              "      <th>Frequency_Stanford_NER</th>\n",
              "      <th>Frequency_Spacy_NER</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Jesus</td>\n",
              "      <td>1190</td>\n",
              "      <td>1347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>John</td>\n",
              "      <td>889</td>\n",
              "      <td>1058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>David</td>\n",
              "      <td>851</td>\n",
              "      <td>996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Paul</td>\n",
              "      <td>598</td>\n",
              "      <td>653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Mike</td>\n",
              "      <td>457</td>\n",
              "      <td>586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>Larson</td>\n",
              "      <td>86</td>\n",
              "      <td>87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>Bruce</td>\n",
              "      <td>85</td>\n",
              "      <td>87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>Clayton</td>\n",
              "      <td>84</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>Sam</td>\n",
              "      <td>84</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>Adams</td>\n",
              "      <td>83</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>75 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   PERSON Entity  Frequency_Stanford_NER  Frequency_Spacy_NER\n",
              "0          Jesus                    1190                 1347\n",
              "1           John                     889                 1058\n",
              "2          David                     851                  996\n",
              "3           Paul                     598                  653\n",
              "4           Mike                     457                  586\n",
              "..           ...                     ...                  ...\n",
              "70        Larson                      86                   87\n",
              "71         Bruce                      85                   87\n",
              "72       Clayton                      84                   86\n",
              "73           Sam                      84                   86\n",
              "74         Adams                      83                   84\n",
              "\n",
              "[75 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBniz2MirWHY"
      },
      "source": [
        "df_m_loc.to_csv('Top_100_matched_LOC.csv') #saving the matched LOCATION entity\n",
        "df_m_person.to_csv('Top_100_matched_PERSON.csv') #saving the matched PERSON entity "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7wneYzZohLX",
        "outputId": "33e05683-f8fa-4adc-9a82-d190861db592"
      },
      "source": [
        "print(\"Degree of correlation between LOC tags from stanford NER and Spacy NER is {}\".format(count_loc/100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Degree of correlation between LOC tags from stanford NER and Spacy NER is 0.71\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDsQNpOuo3CJ",
        "outputId": "a726a0ba-1c39-439b-a924-8b33034d4924"
      },
      "source": [
        "print(\"Degree of correlation between PERSON tags from stanford NER and Spacy NER is {}\".format(count_person/100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Degree of correlation between PERSON tags from stanford NER and Spacy NER is 0.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-H-7KK-rSMW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}